{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Pre-Proccessing --->\n",
        "\n",
        "This code is performing text preprocessing, a common step in Natural Language Processing (NLP) to prepare text data for further analysis.\n",
        "Let's break down the code in detail and introduce some questions you may be asked in an oral examination, along with answers.\n",
        "\n",
        "Code Explanation\n",
        "\n",
        "Installation and Importing NLTK Library\n",
        "\n",
        "python\n",
        "Copy code\n",
        "pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "pip install nltk: Installs the Natural Language Toolkit (NLTK), a library for working with human language data in Python.\n",
        "nltk.download('punkt'): Downloads the Punkt tokenizer models, required for tokenizing text into sentences and words.\n",
        "\n",
        "Importing Modules and Tools for Text Processing\n",
        "\n",
        "python\n",
        "Copy code\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "stopwords: Provides a list of common words in English that typically don’t contribute to the meaning, like \"is\", \"and\", \"the\".\n",
        "PorterStemmer: This is a stemming algorithm that reduces words to their root form (e.g., \"running\" to \"run\").\n",
        "word_tokenize and sent_tokenize: Tokenizers for breaking text into words and sentences, respectively.\n",
        "\n",
        "Sample Text Initialization\n",
        "\n",
        "python\n",
        "Copy code\n",
        "random_text = \"\"\" \"\"\"\n",
        "\n",
        "This variable is expected to contain text data. You would replace the placeholder with actual text data for analysis.\n",
        "Tokenizing Text into Words\n",
        "\n",
        "python\n",
        "Copy code\n",
        "words = word_tokenize(random_text)\n",
        "\n",
        "word_tokenize converts random_text into a list of individual words. Tokenization is essential for breaking down text data into manageable units for further processing.\n",
        "Downloading and Initializing Stop Words\n",
        "\n",
        "python\n",
        "Copy code\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "stopwords.words(\"english\"): Fetches English stop words to filter out non-informative words from the text.\n",
        "Preprocessing Each Word\n",
        "\n",
        "python\n",
        "Copy code\n",
        "preprocessed_words = []\n",
        "\n",
        "for word in words:\n",
        "    word = word.lower()\n",
        "    word = word.strip('.,?!-()[]{}\"\\'')\n",
        "    if word not in stop_words:\n",
        "        word = stemmer.stem(word)\n",
        "        preprocessed_words.append(word)\n",
        "\n",
        "word.lower(): Converts each word to lowercase, ensuring uniformity.\n",
        "word.strip('.,?!-()[]{}\"\\''): Removes punctuation from the word, making it easier to process.\n",
        "if word not in stop_words: Filters out any words that are stop words.\n",
        "stemmer.stem(word): Applies the Porter Stemming algorithm to reduce words to their base form.\n",
        "preprocessed_words.append(word): Adds the processed word to the preprocessed_words list.\n",
        "Combining Preprocessed Words into a Text\n",
        "\n",
        "python\n",
        "Copy code\n",
        "preprocessed_text = \" \".join(preprocessed_words)\n",
        "\n",
        "Joins all the preprocessed words back into a single string, representing the cleaned text.\n",
        "Printing the Results\n",
        "\n",
        "python\n",
        "Copy code\n",
        "print(\"Original Text:\")\n",
        "print(random_text)\n",
        "print(\"\\nPreprocessed Text:\")\n",
        "print(preprocessed_text)\n",
        "\n",
        "Prints both the original and preprocessed text for comparison.\n",
        "\n",
        "\n",
        "Key Concepts Behind the Code\n",
        "Tokenization: Dividing text into smaller units (tokens) like words or sentences. This is crucial for further analysis and helps algorithms focus on individual words.\n",
        "\n",
        "Stop Words: Commonly used words that add little meaning in text analysis. Filtering these out improves the efficiency of NLP algorithms.\n",
        "\n",
        "Stemming: Reduces words to their root forms (e.g., \"working\" becomes \"work\") to standardize words and reduce redundancy in analysis.\n",
        "\n",
        "\n",
        "Potential Oral Examination Questions and Answers\n",
        "Q: What is tokenization, and why is it important in NLP?\n",
        "A: Tokenization is the process of splitting text into smaller components like words or sentences. It is essential because it allows NLP algorithms to focus on individual units (tokens), making it easier to analyze and process language data accurately.\n",
        "\n",
        "Q: Why do we remove stop words from the text?\n",
        "A: Stop words are common words that typically don’t add significant meaning to text data, like \"is\", \"the\", and \"and\". Removing them reduces noise in the data and helps NLP models focus on more informative words.\n",
        "\n",
        "Q: What is the purpose of stemming, and how does the Porter Stemmer work?\n",
        "A: Stemming reduces words to their base or root form, which standardizes variations of a word (e.g., \"jumps\", \"jumping\" all become \"jump\"). The Porter Stemmer applies a series of rules to remove suffixes and reduce a word to its stem.\n",
        "\n",
        "Q: Why do we lowercase and remove punctuation in the text preprocessing step?\n",
        "A: Lowercasing ensures that different cases of a word (e.g., \"Apple\" vs. \"apple\") are treated the same, and removing punctuation helps in cleaning up the text, making it easier to process without irrelevant characters.\n",
        "\n",
        "Q: Can you explain the significance of the nltk.download('punkt') line?\n",
        "A: This line downloads the Punkt tokenizer models, which are required for breaking down text into words and sentences. Without this, the word and sentence tokenizers in NLTK wouldn’t function.\n",
        "\n",
        "Q: What is the difference between stemming and lemmatization?\n",
        "A: Stemming reduces words to their base form by chopping off suffixes, often resulting in non-standard words (e.g., \"running\" to \"run\"). Lemmatization, on the other hand, reduces words to their root form based on their meaning, producing grammatically correct words (e.g., \"better\" to \"good\").\n",
        "\n",
        "Q: How would you modify this code to perform lemmatization instead of stemming?\n",
        "A: To perform lemmatization, we would import WordNetLemmatizer from NLTK, initialize it, and replace stemmer.stem(word) with lemmatizer.lemmatize(word). We would also need to download the WordNet dataset using nltk.download('wordnet').\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "g3N9tnYavuko"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Email Spam Detection --->\n",
        "\n",
        "This code is an implementation of a spam detection classifier using machine learning. It leverages the Support Vector Machine (SVM) algorithm to classify emails as either \"Spam\" or \"Not Spam.\"\n",
        "Here’s a breakdown of the code, along with potential questions and answers that could be asked during an oral examination.\n",
        "\n",
        "Code Explanation\n",
        "Importing Libraries\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import re\n",
        "\n",
        "pandas: Used for data manipulation and reading the dataset.\n",
        "TfidfVectorizer: Converts text data into numerical features using Term Frequency-Inverse Document Frequency (TF-IDF).\n",
        "train_test_split: Splits the dataset into training and testing sets.\n",
        "SVC: The Support Vector Classifier from the sklearn library used for training the spam detection model.\n",
        "accuracy_score and classification_report: Metrics to evaluate the performance of the classifier.\n",
        "re: Provides support for regular expressions, used for cleaning email subjects.\n",
        "\n",
        "Loading the Dataset\n",
        "\n",
        "python\n",
        "Copy code\n",
        "data = pd.read_csv('emails.csv')\n",
        "This loads the dataset from a CSV file, where each row represents an email with text content and a label indicating whether it is spam (1) or not spam (0).\n",
        "\n",
        "Splitting Data into Features and Labels\n",
        "\n",
        "python\n",
        "Copy code\n",
        "X = data['text']\n",
        "y = data['spam']\n",
        "X contains the email text content, while y contains the target labels (spam or not spam).\n",
        "\n",
        "Converting Text Data to Numerical Features with TF-IDF\n",
        "\n",
        "python\n",
        "Copy code\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_vectorized = vectorizer.fit_transform(X)\n",
        "TfidfVectorizer converts the email text into numerical form by calculating the importance of each word in each email relative to the whole dataset.\n",
        "This transformation allows us to use text data as input for the machine learning model.\n",
        "\n",
        "Splitting the Data into Training and Testing Sets\n",
        "\n",
        "python\n",
        "Copy code\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
        "This splits the vectorized data into training and testing sets, using 80% for training and 20% for testing. random_state=42 ensures reproducibility of the split.\n",
        "\n",
        "Creating and Training the SVM Classifier\n",
        "\n",
        "python\n",
        "Copy code\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "SVC(kernel='linear'): Creates a Support Vector Machine (SVM) classifier with a linear kernel, suitable for binary classification tasks.\n",
        "svm_classifier.fit(X_train, y_train): Trains the classifier on the training data.\n",
        "\n",
        "Making Predictions and Evaluating the Model\n",
        "\n",
        "python\n",
        "Copy code\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "y_pred: Contains the predictions made by the classifier on the test set.\n",
        "accuracy_score(y_test, y_pred): Computes the accuracy of the classifier.\n",
        "\n",
        "Displaying the Classification Report\n",
        "\n",
        "python\n",
        "Copy code\n",
        "class_report = classification_report(y_test, y_pred, target_names=['Not Spam', 'Spam'])\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "The classification report shows the precision, recall, and F1-score for each class (Not Spam and Spam), providing a detailed evaluation of the model’s performance.\n",
        "\n",
        "Defining a Function to Classify Email Subjects\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def classify_email(subject):\n",
        "    cleaned_subject = re.sub(r'^Subject:\\s*', '', subject)\n",
        "    vectorized_subject = vectorizer.transform([cleaned_subject])\n",
        "    prediction = svm_classifier.predict(vectorized_subject)\n",
        "    return \"Spam\" if prediction[0] == 1 else \"Not Spam\"\n",
        "This function takes an email subject as input, removes any \"Subject:\" prefix using regular expressions, transforms it using the TF-IDF vectorizer, and then classifies it using the SVM model.\n",
        "\n",
        "Getting User Input and Classifying the Email Subject\n",
        "\n",
        "python\n",
        "Copy code\n",
        "user_input = input(\"Enter an email subject: \")\n",
        "classification_result = classify_email(user_input)\n",
        "print(\"Classification:\", classification_result)\n",
        "The user is prompted to enter an email subject, which is then classified as \"Spam\" or \"Not Spam.\"\n",
        "\n",
        "Key Concepts Behind the Code\n",
        "TF-IDF Vectorization: A technique that transforms text data into numerical form by assigning weights to words based on their importance in the text. Higher weights are assigned to unique words, and common words receive lower weights.\n",
        "\n",
        "Support Vector Machine (SVM): A supervised learning algorithm used for classification. The linear kernel helps separate data points in high-dimensional spaces by finding the optimal hyperplane.\n",
        "\n",
        "Classification Metrics: Accuracy, precision, recall, and F1-score are used to evaluate the model's performance on unseen data.\n",
        "\n",
        "\n",
        "Potential Oral Examination Questions and Answers\n",
        "Q: What is TF-IDF, and why is it used in this code?\n",
        "A: TF-IDF (Term Frequency-Inverse Document Frequency) is a technique that transforms text into numerical data by giving higher weights to unique words and lower weights to common words. In this code, it is used to convert email text into a format suitable for input into the SVM model.\n",
        "\n",
        "Q: Why do we use train-test split in machine learning?\n",
        "A: Train-test split divides the dataset into training and testing subsets. The training set is used to teach the model, while the test set evaluates its performance on unseen data, helping to detect overfitting and generalization.\n",
        "\n",
        "Q: What is the purpose of using an SVM with a linear kernel in this code?\n",
        "A: The SVM with a linear kernel is used for binary classification (spam vs. not spam) because it efficiently separates the data into two classes using a linear decision boundary.\n",
        "\n",
        "Q: Explain precision, recall, and F1-score. Why are these metrics important?\n",
        "A: Precision is the percentage of correctly identified spam emails out of all predicted spam. Recall is the percentage of actual spam emails correctly identified. The F1-score is the harmonic mean of precision and recall, balancing both. These metrics help us understand the model’s effectiveness beyond simple accuracy.\n",
        "\n",
        "Q: Why is it important to clean the email subject before classification?\n",
        "A: Cleaning the email subject removes unnecessary text, like the \"Subject:\" prefix, which could introduce irrelevant data. This helps the model focus on the meaningful content of the subject.\n",
        "\n",
        "Q: What would you modify to use this code with a different dataset?\n",
        "A: To use a different dataset, replace emails.csv with the new dataset file path, ensuring that the new file has similar column names (\"text\" for email content and \"spam\" for labels) or adjust the code to match the new column names.\n",
        "\n",
        "Q: How would you handle a case where you want to classify an email body instead of the subject?\n",
        "A: Replace the subject input in the classify_email function with the entire email body. Additionally, you may need to apply further preprocessing steps to clean and standardize the email body text.\n",
        "\n",
        "Q: Can you explain the role of the regular expression used in the classify_email function?\n",
        "A: The regular expression re.sub(r'^Subject:\\s*', '', subject) removes any \"Subject:\" prefix from the input, ensuring that the classification focuses solely on the content of the email subject.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZCZpf4VkZGwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Agglomerative Clustering --->\n",
        "\n",
        "This code applies hierarchical clustering to a credit card dataset to identify distinct customer segments based on selected features.\n",
        "It uses Agglomerative Clustering and visualizes the clustering results with both a dendrogram and a scatter plot.\n",
        "\n",
        "Code Explanation\n",
        "Importing Libraries\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "numpy: Useful for numerical operations, though not used directly in this code.\n",
        "pandas: Loads and handles the dataset.\n",
        "matplotlib.pyplot: Used to create plots for data visualization.\n",
        "StandardScaler: Scales features to have zero mean and unit variance, which is essential for clustering.\n",
        "AgglomerativeClustering: Performs hierarchical clustering on standardized data.\n",
        "dendrogram and linkage: Create and display the hierarchical structure of clusters.\n",
        "\n",
        "Loading the Dataset\n",
        "\n",
        "python\n",
        "Copy code\n",
        "data = pd.read_csv('BankChurners.csv')\n",
        "This loads a dataset of credit card customers, stored as BankChurners.csv.\n",
        "\n",
        "Selecting Relevant Features\n",
        "\n",
        "python\n",
        "Copy code\n",
        "X = data[['Customer_Age', 'Dependent_count', 'Months_on_book', 'Total_Relationship_Count', 'Months_Inactive_12_mon']]\n",
        "Five features are selected to represent each customer, which are likely related to their banking behavior. These features will be used to form clusters.\n",
        "\n",
        "Standardizing the Features\n",
        "\n",
        "python\n",
        "Copy code\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "StandardScaler standardizes the features to ensure that they contribute equally to the clustering process. This avoids bias toward features with larger ranges.\n",
        "\n",
        "Applying Agglomerative Clustering\n",
        "\n",
        "python\n",
        "Copy code\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3)\n",
        "agg_labels = agg_clustering.fit_predict(X_scaled)\n",
        "AgglomerativeClustering performs hierarchical clustering by iteratively merging clusters. Setting n_clusters=3 instructs it to create three clusters.\n",
        "fit_predict returns cluster labels for each data point, stored in agg_labels.\n",
        "\n",
        "Creating and Displaying the Dendrogram\n",
        "\n",
        "python\n",
        "Copy code\n",
        "linked = linkage(X_scaled, 'ward')\n",
        "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
        "plt.show()\n",
        "linkage(X_scaled, 'ward') computes the hierarchical clustering using the Ward method, which minimizes the variance within clusters as they merge.\n",
        "dendrogram generates a visual representation of the clustering process, showing the hierarchical structure and possible clusters at various levels.\n",
        "\n",
        "Plotting the Clusters in 2D\n",
        "\n",
        "python\n",
        "Copy code\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=agg_labels, cmap='rainbow')\n",
        "plt.xlabel('Standardized Customer Age')\n",
        "plt.ylabel('Standardized Dependent Count')\n",
        "plt.show()\n",
        "This scatter plot visualizes clusters in two dimensions, using 'Customer Age' and 'Dependent Count' to represent each customer. Colors indicate the clusters assigned by the model.\n",
        "\n",
        "Key Concepts Behind the Code\n",
        "Hierarchical Clustering: Hierarchical clustering is a type of unsupervised learning that builds a hierarchy of clusters.\n",
        "In this case, agglomerative (bottom-up) clustering is used, where individual points are progressively merged into clusters.\n",
        "\n",
        "Ward’s Method: This is a variance-minimizing approach used in hierarchical clustering to merge clusters.\n",
        "It attempts to minimize the total within-cluster variance.\n",
        "\n",
        "Dendrogram: A dendrogram is a tree-like diagram representing hierarchical relationships. It shows how clusters are formed at each level,\n",
        "allowing the user to decide on the optimal number of clusters visually.\n",
        "\n",
        "Standardization: Standardizing the features is critical in clustering as it ensures that each feature contributes equally,\n",
        "preventing any single feature from dominating due to its scale.\n",
        "\n",
        "\n",
        "Potential Oral Examination Questions and Answers\n",
        "Q: What is hierarchical clustering, and why do we use it here?\n",
        "A: Hierarchical clustering is a method of creating a nested sequence of clusters by either merging smaller clusters or splitting larger ones. We use it here to explore different customer segments and understand relationships at multiple levels of hierarchy.\n",
        "\n",
        "Q: Explain the role of the StandardScaler in this code.\n",
        "A: StandardScaler standardizes each feature by centering it to have a mean of zero and a standard deviation of one. This step is essential in clustering to ensure that all features contribute equally to the distance metric used for clustering.\n",
        "\n",
        "Q: What is the purpose of the dendrogram in this code?\n",
        "A: The dendrogram visually represents the hierarchical clustering process, showing the order in which clusters are merged and allowing us to decide on the optimal number of clusters by examining cluster distances.\n",
        "\n",
        "Q: Why do we use the Ward method in linkage for creating the dendrogram?\n",
        "A: The Ward method minimizes the variance within clusters as they merge, which helps produce compact clusters and reduces the variance within each cluster.\n",
        "\n",
        "Q: Why might we select a different number of clusters when using hierarchical clustering?\n",
        "A: The dendrogram shows clustering at multiple levels. By observing it, we can decide on the number of clusters based on where the largest distance jumps occur, indicating a natural split.\n",
        "\n",
        "Q: Can you explain the difference between agglomerative and divisive clustering?\n",
        "A: Agglomerative clustering starts with each data point as its own cluster and merges them iteratively, while divisive clustering starts with all data points in a single cluster and splits them iteratively.\n",
        "\n",
        "Q: What does fit_predict do in the AgglomerativeClustering model?\n",
        "A: fit_predict fits the model to the data and returns an array of cluster labels for each data point, allowing us to visualize or analyze the assigned clusters.\n",
        "\n",
        "Q: Why are only two features (Customer_Age and Dependent_count) used in the 2D scatter plot?\n",
        "A: Visualizing all five features at once is challenging in 2D. By selecting two of the most relevant features, we create a simplified plot to observe clustering trends, though this limits the representation of clusters to only these two features.\n",
        "\n",
        "Q: How would you determine the optimal number of clusters in hierarchical clustering?\n",
        "A: By examining the dendrogram, we look for the largest vertical distances (gaps) between clusters, as these indicate natural points to cut the tree and select clusters.\n",
        "\n",
        "Q: If you were to use this code on a different dataset, what modifications would you make?\n",
        "A: I would change the data = pd.read_csv(...) line to load the new dataset and adjust the X variable to include relevant features from the new data. Additionally, the plot labels may need to be updated to match the new features used.\n",
        "\n",
        "Q: Can you use other clustering methods with this dataset? If so, which ones?\n",
        "A: Yes, other clustering methods like K-means or DBSCAN could be applied to this dataset, though they do not produce a dendrogram and may handle different data distributions or density patterns differently.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "RDu9W-7gaU-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Inverted-files --->\n",
        "\n",
        "This code builds an inverted index for a set of sample documents and performs simple document retrieval based on a query.\n",
        "\n",
        "Code Explanation\n",
        "Importing Libraries\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import re\n",
        "import collections\n",
        "re: This library is used for regular expressions, which help in text processing.\n",
        "collections: Provides a defaultdict, which is used here to create the inverted index.\n",
        "\n",
        "Sample Documents\n",
        "\n",
        "python\n",
        "Copy code\n",
        "documents = {\n",
        "    1: \"This is the first document. It contains some words.\",\n",
        "    2: \"This is the second document. It also contains words.\",\n",
        "    3: \"The third document is different from the first two.\",\n",
        "    4: \"Inverted index is essential for document retrieval.\",\n",
        "}\n",
        "A dictionary of sample documents where keys are document IDs and values are document text.\n",
        "\n",
        "Preprocessing and Tokenization\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    tokens = re.findall(r'\\w+', text)\n",
        "    return tokens\n",
        "This function converts the text to lowercase for case-insensitive matching and uses re.findall to extract words (alphanumeric characters) as tokens.\n",
        "\n",
        "Building the Inverted Index\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def build_inverted_index(documents):\n",
        "    inverted_index = collections.defaultdict(list)\n",
        "    for doc_id, document in documents.items():\n",
        "        tokens = preprocess(document)\n",
        "        for token in tokens:\n",
        "            inverted_index[token].append(doc_id)\n",
        "    return inverted_index\n",
        "This function creates an inverted index, which maps each unique word (token) to a list of document IDs where the word appears.\n",
        "defaultdict(list) automatically initializes empty lists for new tokens, allowing easy insertion of document IDs.\n",
        "\n",
        "Retrieving Documents\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def retrieve_documents(query, inverted_index):\n",
        "    query_tokens = preprocess(query)\n",
        "    result = set()\n",
        "\n",
        "    # Retrieve documents containing each query token\n",
        "    for token in query_tokens:\n",
        "        if token in inverted_index:\n",
        "            if not result:\n",
        "                result = set(inverted_index[token])\n",
        "            else:\n",
        "                result = result.intersection(inverted_index[token])\n",
        "\n",
        "    return result\n",
        "This function takes a query, tokenizes it, and finds documents containing all the query tokens (AND operation).\n",
        "It uses set operations to perform intersections, so the result contains only document IDs that have all the query tokens.\n",
        "\n",
        "Building the Inverted Index and Testing with a Query\n",
        "\n",
        "python\n",
        "Copy code\n",
        "inverted_index = build_inverted_index(documents)\n",
        "query1 = input(\"Enter query: \")\n",
        "result1 = retrieve_documents(query1, inverted_index)\n",
        "print(\"Query:\", query1)\n",
        "print(\"Matching Documents:\", result1)\n",
        "build_inverted_index generates the index for the sample documents.\n",
        "retrieve_documents returns document IDs matching the input query. For example, a query like \"first document\" would return documents containing both \"first\" and \"document.\"\n",
        "\n",
        "Key Concepts Behind the Code\n",
        "Inverted Index: An inverted index maps each unique word in a collection to the list of documents where it appears. It is commonly used in search engines for efficient document retrieval.\n",
        "\n",
        "Preprocessing: Lowercasing and tokenizing text simplifies matching by ensuring case-insensitive searches and removing non-alphanumeric characters.\n",
        "\n",
        "Set Intersection for Query Matching: When a query has multiple words, this code finds documents that contain all query words by intersecting sets of document IDs for each word.\n",
        "\n",
        "\n",
        "Potential Oral Examination Questions and Answers\n",
        "Q: What is an inverted index, and why is it useful in document retrieval?\n",
        "A: An inverted index maps each unique word in a document collection to a list of documents where it appears. This structure is useful for quick document retrieval, as it allows us to look up documents containing specific words or phrases.\n",
        "\n",
        "Q: How does the preprocess function work? Why do we use regular expressions here?\n",
        "A: preprocess converts the text to lowercase and uses re.findall(r'\\w+', text) to extract words, ensuring consistent case-insensitive matching and removing punctuation. Regular expressions help capture only the alphanumeric parts of the text as tokens.\n",
        "\n",
        "Q: Explain the logic behind the retrieve_documents function.\n",
        "A: retrieve_documents tokenizes the query and retrieves documents for each token from the inverted index. It intersects document lists for each token to return documents containing all query words, implementing an \"AND\" search.\n",
        "\n",
        "Q: Why is defaultdict(list) used in building the inverted index?\n",
        "A: defaultdict(list) allows us to initialize a new list automatically for each token that doesn’t yet exist in the dictionary. This simplifies code by avoiding the need to check for the existence of a token before appending a document ID.\n",
        "\n",
        "Q: How would the retrieval change if we wanted an \"OR\" search instead of an \"AND\" search?\n",
        "A: For an \"OR\" search, we would use a union of document lists for each query token rather than an intersection, meaning we would return documents containing at least one of the query words.\n",
        "\n",
        "Q: If a document contains a query word multiple times, how is this handled in this code?\n",
        "A: This code does not consider word frequency; it only checks for the presence of a word in a document. Document IDs are added once per document, regardless of how often a word appears.\n",
        "\n",
        "Q: What is the time complexity of the document retrieval function?\n",
        "A: Document retrieval’s time complexity is proportional to the number of query tokens and the length of the document lists in the inverted index. Finding intersections may take time but is generally efficient due to Python’s set operations.\n",
        "\n",
        "Q: Why are sets used for result in retrieve_documents?\n",
        "A: Sets allow efficient intersection operations, which are necessary for the \"AND\" search functionality in this function, and they ensure that each document ID appears only once in the result.\n",
        "\n",
        "Q: How would you modify this code to handle phrase searches (e.g., \"first document\")?\n",
        "A: For phrase searches, we would need to store and retrieve not just document IDs but also the positions of each token within documents, then check if the tokens appear sequentially.\n",
        "\n",
        "Q: What modifications would you make to this code to work with a large dataset?\n",
        "A: For larger datasets, we could optimize by using more memory-efficient data structures or indexing techniques like Apache Lucene. We might also consider storing the inverted index on disk with efficient I/O operations for retrieval.\n",
        "\n",
        "Q: Can this code handle partial word searches (like searching for \"doc\" instead of \"document\")?\n",
        "A: No, this code only matches complete tokens. To support partial matching, we would need to modify the build_inverted_index function to index word stems or prefixes.\n",
        "\n",
        "Q: Why are regular expressions important in text preprocessing?\n",
        "A: Regular expressions allow precise extraction and manipulation of text patterns, which is essential for breaking down sentences into words and removing non-alphanumeric characters effectively.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x4WwjRf3aU63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "PageRank Algorithm --->\n",
        "\n",
        "This code calculates the PageRank of pages in a web graph using the iterative PageRank algorithm.\n",
        "\n",
        "Code Explanation\n",
        "Importing NumPy Library\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import numpy as np\n",
        "This library is used for numerical operations and handling arrays efficiently.\n",
        "\n",
        "PageRank Function\n",
        "\n",
        "python\n",
        "Copy code\n",
        "def page_rank(graph, damping_factor=0.85, max_iterations=100, tol=1e-6):\n",
        "    # Number of pages\n",
        "    num_pages = len(graph)\n",
        "\n",
        "    # Initialize the PageRank values\n",
        "    pagerank = np.ones(num_pages) / num_pages\n",
        "page_rank is a function that calculates PageRank values for pages in a web graph.\n",
        "graph is the adjacency matrix representing links between pages.\n",
        "damping_factor simulates the probability that a user continues clicking on links (default is 0.85).\n",
        "max_iterations limits the number of times the algorithm runs if convergence is not reached.\n",
        "tol is the tolerance level for convergence (when the change in values is smaller than this, the algorithm stops).\n",
        "\n",
        "Main Iteration Loop\n",
        "\n",
        "python\n",
        "Copy code\n",
        "for _ in range(max_iterations):\n",
        "    new_pagerank = np.zeros(num_pages)\n",
        "    for i in range(num_pages):\n",
        "        for j in range(num_pages):\n",
        "            if graph[j][i]:\n",
        "                new_pagerank[i] += pagerank[j] / sum(graph[j])\n",
        "For each page, new_pagerank is calculated based on the sum of incoming PageRank values from pages linking to it.\n",
        "If graph[j][i] is 1, page j links to page i, contributing its PageRank proportionally based on its outlinks.\n",
        "\n",
        "Applying the Damping Factor\n",
        "\n",
        "python\n",
        "Copy code\n",
        "new_pagerank = (1 - damping_factor) / num_pages + damping_factor * new_pagerank\n",
        "After calculating new_pagerank for all pages, it is adjusted by the damping factor, ensuring that some probability (e.g., 0.15) is assigned to \"randomly\" visiting any page.\n",
        "\n",
        "Checking for Convergence\n",
        "\n",
        "python\n",
        "Copy code\n",
        "if np.linalg.norm(new_pagerank - pagerank) < tol:\n",
        "    return new_pagerank\n",
        "pagerank = new_pagerank\n",
        "If the difference between new_pagerank and the previous pagerank is smaller than tol, the algorithm assumes convergence and returns new_pagerank as the final PageRank values.\n",
        "If not, pagerank is updated, and the loop continues.\n",
        "\n",
        "Example Graph and Running the Function\n",
        "\n",
        "python\n",
        "Copy code\n",
        "web_graph = [\n",
        "    [0, 1, 1, 0],\n",
        "    [0, 0, 1, 0],\n",
        "    [1, 0, 0, 1],\n",
        "    [0, 0, 1, 0]\n",
        "]\n",
        "pagerank_values = page_rank(web_graph)\n",
        "print(\"PageRank values:\", pagerank_values)\n",
        "web_graph represents a sample set of web pages where 1 indicates a link between pages. This graph is passed to the page_rank function to calculate PageRank values, which are then printed.\n",
        "\n",
        "Key Concepts Behind the Code\n",
        "PageRank: A link-based algorithm for ranking web pages. Pages with more and higher-quality links have higher PageRank.\n",
        "Damping Factor: Simulates the chance a user continues clicking links, with a probability (usually 0.85) and randomly selects a page otherwise.\n",
        "Convergence: When the algorithm stabilizes, meaning further iterations cause negligible change in PageRank values.\n",
        "\n",
        "\n",
        "Potential Oral Examination Questions and Answers\n",
        "Q: What is the purpose of the PageRank algorithm?\n",
        "A: PageRank assigns a numerical weight to each web page to measure its relative importance based on the structure of links across the web. Higher PageRank indicates a more influential or authoritative page.\n",
        "\n",
        "Q: Explain the role of the damping factor in the PageRank algorithm.\n",
        "A: The damping factor represents the probability that a user continues clicking links rather than randomly jumping to another page. A typical value of 0.85 assumes an 85% chance of following links and 15% chance of randomly visiting a page.\n",
        "\n",
        "Q: Why do we check for convergence in the PageRank function?\n",
        "A: We check for convergence to ensure that the algorithm stops once PageRank values stabilize, preventing unnecessary computations. Convergence occurs when the difference between current and previous PageRank values falls below the tolerance (tol).\n",
        "\n",
        "Q: How would the algorithm change if we set the damping factor to 1?\n",
        "A: With a damping factor of 1, the algorithm assumes that users only follow links without random jumps, making the PageRank calculation dependent solely on the link structure without any random factor.\n",
        "\n",
        "Q: What is the time complexity of this PageRank algorithm?\n",
        "A: The time complexity is approximately O(N^2 * max_iterations), where N is the number of pages, as each iteration involves checking each page's links to other pages.\n",
        "\n",
        "Q: Why is the PageRank initialized equally across pages?\n",
        "A: Initially, we assume all pages are equally important; this uniform start helps the algorithm adjust ranks solely based on the link structure over successive iterations.\n",
        "\n",
        "Q: What would happen if there is a page with no outbound links? How is it handled?\n",
        "A: In this implementation, if a page has no outbound links, it could create a \"sink\" where all PageRank flows into that page. To handle this, we might modify the graph or redistribute the PageRank equally to all pages in such cases.\n",
        "\n",
        "Q: Can the PageRank algorithm be used outside of web search engines?\n",
        "A: Yes, PageRank can be applied in social networks, citation networks, recommendation systems, and any domain where the importance of nodes depends on their connections.\n",
        "\n",
        "Q: Explain why np.linalg.norm is used in the convergence check.\n",
        "A: np.linalg.norm calculates the Euclidean distance between the current and previous PageRank vectors. A small norm value indicates minor changes between iterations, implying convergence.\n",
        "\n",
        "Q: How would you modify the code to handle a larger graph efficiently?\n",
        "A: For large graphs, we might store the adjacency matrix as a sparse matrix to save memory. Additionally, implementing parallel processing or using a distributed computing framework like Apache Spark could improve efficiency.\n",
        "\n",
        "Q: Why is the new_pagerank vector initialized to zeros at each iteration?\n",
        "A: new_pagerank starts at zero to accumulate contributions from each linked page in each iteration. Resetting it prevents carrying over previous values.\n",
        "\n",
        "Q: How does the algorithm handle pages that have no incoming links?\n",
        "A: Pages with no incoming links get a baseline PageRank due to the (1 - damping_factor) term, representing the chance of arriving there randomly.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MJf5ykiZaU4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PIflrgaMaU1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xHobHedCaUyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EZbjy_wqaUvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9XhRHVGaUtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZeBMk6OmaUqL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}